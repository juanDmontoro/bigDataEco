{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Regularization in the linear model](#toc1_)    \n",
    "  - [The dataset](#toc1_1_)    \n",
    "    - [Data loading and characterization](#toc1_1_1_)    \n",
    "    - [Separating features and response](#toc1_1_2_)    \n",
    "    - [Creating a training/test partition](#toc1_1_3_)    \n",
    "  - [Regularization in the linear regression](#toc1_2_)    \n",
    "    - [Regression in high dimensions](#toc1_2_1_)    \n",
    "    - [Regularization: Lasso, ridge and elastic net](#toc1_2_2_)    \n",
    "  - [Benchmark: linear regression (OLS)](#toc1_3_)    \n",
    "  - [Ridge regression](#toc1_4_)    \n",
    "    - [Visualizing coefficients paths](#toc1_4_1_)    \n",
    "    - [Selection of the tuning parameter through cross-validation](#toc1_4_2_)    \n",
    "    - [Estimating the test error](#toc1_4_3_)    \n",
    "  - [LASSO regression](#toc1_5_)    \n",
    "    - [Estimated paths](#toc1_5_1_)    \n",
    "    - [Tuning the model through CV](#toc1_5_2_)    \n",
    "    - [Test error estimation](#toc1_5_3_)    \n",
    "  - [YOUR TURN](#toc1_6_)    \n",
    "  - [YOUR TURN 2](#toc1_7_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Regularization in the linear model](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the necessary libraries for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next three lines load complete libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# This one imports just one module (pyplot) from library matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Then, we import specific functions from different sklearn modules \n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, LassoCV, RidgeCV, ElasticNet, ElasticNetCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Finally we set a filter to avoid verbosity in warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# and a random seed to ensure reproducibility \n",
    "# Note setting the random seed at this point requires the notebook to be run sequentially (keeping always the same order of code lines)\n",
    "random.seed(1) # This sets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[The dataset](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_1_'></a>[Data loading and characterization](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a synthetic dataset with 400 predictors and a reponse variable. The coefficient structure of the underlying model is sparse (most coefficients are approximately zero) with only a few having an impact on the response. We load the dataset using the `read_csv` function in pandas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'juandmontoro.github.io/bigDataEco/data/regularized_regression.csv'\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also load the coefficients that generated the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = pd.read_csv('betas.csv')\n",
    "betas['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features are sorted so the first one has the most impact on the response and it decays "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the decaying pattern of the actual coefficients of the DGM and see that less than 30 of them are greater than $10^{-2}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(betas['0'])\n",
    "plt.yscale('log')\n",
    "plt.axhline(y=10**(-2),color='r',linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(betas['0']>10**-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_2_'></a>[Separating features and response](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us split the dataframe into design matrix and response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.filter(like='Feature')\n",
    "y = data.filter(like='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the resulting design matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_3_'></a>[Creating a training/test partition](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We partition the dataset into training and test (20%) samples. To do so we use the Python-feature called \"tuple unpacking\". When a function returns  a tuple (which is an iterable) you can assign multiple variables at once on the left-hand side of an assignment. See the code next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that the data has been shuffled (but indices in both features and response match) and only 120 observations have been selected. You can check that `X_test` and `y_test` also match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Regularization in the linear regression](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_1_'></a>[Regression in high dimensions](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Note** the regression problem we are considering is to estimate the conditional mean of the response,  $$E[Y|X]=\\hat{f}(X)$$ is is a high-dimensional  one, as $k>>n$. This would fail in the standard linear regression setting as there are many (infinite) regression lines that could be fit to the dataset (think of fitting a line to a point).\n",
    "\n",
    "Penalized regression can estimate a model even if $k>n$ through **coordinate descent**: the objective function can be minimized iteratively, one parameter (or coordinate) at a time, while keeping all others fixed.\n",
    "\n",
    "How It Works:\n",
    "\n",
    "1. Objective Function: Consider a loss function L(β1,β2,…,βp) to be minimized with respect to β=(β1,β2,…,βp).\n",
    "\n",
    "2. Iterative Updates:\n",
    "    - Fix all coefficients except for one, say βj.\n",
    "    - Minimize f with respect to βj​ while keeping all other coefficients constant.\n",
    "    - Repeat this process for all coordinates (β1,β2,…,βpβ1​,β2​,…,βp​) cyclically or in some order.\n",
    "\n",
    "3. Repeat Until Convergence: The algorithm cycles through all coordinates multiple times until the changes in the coefficients (or the value of the objective function) become negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_2_'></a>[Regularization: Lasso, ridge and elastic net](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Elastic Net is a regularization technique that combines both Lasso (Least Absolute Shrinkage and Selection Operator) and Ridge regression. It achieves this by introducing two penalty terms to the loss function: one for Lasso and one for Ridge. The regularization term is\n",
    "\n",
    "$$\n",
    "\\lambda \\left( \\alpha \\sum_{j=1}^{p} | \\beta_j | + \\frac{1 - \\alpha}{2} \\sum_{j=1}^{p} \\beta_j^2 \\right)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $\\lambda$ is the regularization parameter that controls the overall strength of the penalty.\n",
    "- $\\alpha$ is the mixing parameter that balances between Lasso ($\\alpha = 1$) and Ridge ($\\alpha = 0$).\n",
    "- $\\beta_j$ are the coefficients of the model.\n",
    "\n",
    "Specifically, the Elastic Net penalty is a linear combination of the L1 norm (used in Lasso) and the L2 norm (used in Ridge). When the mixing parameter $\\alpha$ is set to 1, Elastic Net behaves like Lasso, applying only the L1 penalty, which encourages sparsity by shrinking some coefficients exactly to zero. Conversely, when $\\alpha$ is set to 0, it behaves like Ridge regression, applying only the L2 penalty, which shrinks coefficients uniformly but does not enforce sparsity. By tuning $\\alpha$, Elastic Net can balance between these two extremes, leveraging the strengths of both methods.\n",
    "\n",
    "Note: The division by 2 in the term $\\frac{1 - \\alpha}{2}$ is actually a standard convention in the formulation of the Elastic Net penalty. This is done to ensure that the regularization term is properly scaled and comparable to the L1 penalty term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Benchmark: linear regression (OLS)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the in-sample predictive performance (perfect):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_true=y_train,y_pred=lm.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the test error is not that good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_true=y_test,y_pred=lm.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Ridge regression](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression introduces a L2 penalty to the standard least squares loss function.\n",
    "The ridge regression objective function is:\n",
    "$$\n",
    "\\text{Minimize} \\left( \\sum_{i=1}^n (y_i - \\sum_{j=1}^p x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 \\right)\n",
    "$$\n",
    "\n",
    "The term $\\lambda$ is referred to as the penalization parameter. This parameter controls the amount of regularization applied to the model. Specifically, it adds a penalty to the size of the coefficients in the regression model, which helps to prevent overfitting by shrinking the coefficients towards zero. In other words, a larger $\\lambda$ increases the amount of shrinkage, leading to smaller coefficient estimates.\n",
    "\n",
    "As the coefficients change depending on the amount of the penalty we start with a visualization of the coefficients path against the penalization parameter $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_1_'></a>[Visualizing coefficients paths](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a grid of lambda values on which estimate the coefficients. We start creating 100 lambda values (in a log scale):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-5,1,100) # we create 100 values between -1 and 2; then convert each to log10(value) \n",
    "lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we estimate the ridge regression coefficients for the different values of lambda. To do so we need to use the function `ElasticNet()` from the module `sklearn.linear_model`.  Note that a ridge regression is an elastic net regression with L1 (LASSO) regularization set to 0. \n",
    "\n",
    "In addition, note that:\n",
    "\n",
    "- In sklearn regularized regression models the parameter $\\lambda$  is called `alpha` (or `alphas` if more than one value is passed) in `sklearn`. In order to be consistent with our previous discussion we refer to lambdas rather than alphas.\n",
    "- In Elastic net the argument `l1_ratio` can be between 01 and 1; 0 for ridge regression, 1 for LASSO. Any value in between introduces L1+L2 penalty. In this example we will set `l1_ratio` to 0.\n",
    "- To fit a path of ridge regression models we use `path()` which can fit both ridge and lasso, as well as a hybrid mixture, across different $\\lambda$ values\n",
    "- It is good practice to standardize the columns of X in these applications, if the variables are measured in different units, as the ridge or lasso penalty is affected by the magnitude of the coefficients. We will introduce this a later stage. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alphas, coefs, _ = ElasticNet.path(X_train,y_train,l1_ratio =0.,alphas=lambdas) \n",
    "\n",
    "# The argument l1_ratio controls for the type of penalty used\n",
    "# l1_ratio : float, default=0.5 Number between 0 and 1 passed to elastic net (mix of l1 and l2 penalties). \n",
    "# l1_ratio=1 corresponds to the Lasso; \n",
    "# l1_ratio=0 corresponds to ridge regression; \n",
    "\n",
    "\n",
    "# Again we are doing unpacking in this code.\n",
    "#  The path function returns alphas, coeficients and dual gaps (a measure of how close the \n",
    "# current solution is to the optimal solution of the Elastic Net optimization problem).\n",
    "# We only need the former two, hence the use of an underscore _ (means we will not save that value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print both the lambdas (which is unnecessary as they were provided by us) and the associated coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: what is the dimensionality of the coefficients? We have three indexes (the first refers to the response variable, only one in this case; the second to the coefficient, 400; the third to the value of lambda, 100):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to remove the first dimension (as only one response y is considered in this setup):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = coefs[0,:,:]\n",
    "coefs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a dataframe with the solutions path including coefficient values indexed by the lambdas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "soln_path = pd.DataFrame (coefs.T,\n",
    "                           columns=X_train.columns,\n",
    "                           index=np.log10(alphas))\n",
    "soln_path.index.name = 'log(lambda)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soln_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We see that as lambda increases the coefficients are shrinked towards zero!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we plot this path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "ax = soln_path.plot(legend=False)\n",
    "ax.set_xlabel ('$\\log_{10}(\\lambda)$', fontsize =20)\n",
    "ax.set_ylabel ('Coefficients ', fontsize =20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows how the coefficients shrink towards zero as we increase the penalty $\\lambda$ (only for lambda=infinity they become zeroes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_2_'></a>[Selection of the tuning parameter through cross-validation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which value is better? It is difficult to know. To choose the best value we will proceed with cross validation (CV). The ridge, lasso, and elastic net can be efficiently fit along a sequence of λ values, creating what is known as a solution path or regularization path. Hence there is specialized code to fit such paths, and to choose a suitable value of λ using cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgeCV = RidgeCV(alphas=lambdas,      \n",
    "                  cv=5, # we use 5-fold CV; if not set ridge performs LOOCV (solution will likely change)\n",
    "                  scoring='neg_mean_squared_error' # we use -MSE to choose tuning parameter\n",
    "                )\n",
    "ridgeCV.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validated $\\lambda$ is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgeCV.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: the value returned by the above expression is a numpy's double-precision floating-point object. Although unnecessary (this representation is compatible with Python), one can retrieve the number using the method `.item()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgeCV.alpha_.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the set of coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgeCV.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also retrieve the fitted intercept in the final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgeCV.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_3_'></a>[Estimating the test error](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we get the score ($R^2$) and MSE for the trained dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgeCV.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_train,ridgeCV.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But most important, we get the score and mse for the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgeCV.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test,ridgeCV.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could try the process by hand as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test,ridgeCV.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the test error (or the $R^2$ score for that matter) is significantly larger ($R^2$ smaller) than than the training (in-sample) counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us store the different results to generate at the end of the activity a summary table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create empty lists for the different metrics\n",
    "model = []\n",
    "r2_train = []\n",
    "mse_train = []\n",
    "r2_test =[]\n",
    "mse_test = []\n",
    "\n",
    "# Next we append the specific values obtained\n",
    "model.append('Ridge')\n",
    "r2_train.append(ridgeCV.score(X_train,y_train))\n",
    "r2_test.append(ridgeCV.score(X_test,y_test))\n",
    "mse_train.append(mean_squared_error(y_train,ridgeCV.predict(X_train)))\n",
    "mse_test.append(mean_squared_error(y_test,ridgeCV.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[LASSO regression](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_1_'></a>[Estimated paths](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first, see the LASSO coefficient paths for different $\\lambda$ values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-5,1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas, coefs, _ = ElasticNet.path(X_train,y_train,l1_ratio =1,alphas=lambdas)\n",
    "coefs=coefs[0,:,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "soln_path = pd.DataFrame (coefs.T,\n",
    "                           columns=X_train.columns,\n",
    "                           index=np.log10(alphas))\n",
    "soln_path.index.name = 'log Lambda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soln_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see selection in action (more features are forced down to zero as $\\lambda$ increases). Let us take a look at the plot of the paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "ax = soln_path.plot(legend=False)\n",
    "ax.set_xlabel ('$\\log(\\lambda)$', fontsize =20)\n",
    "ax.set_ylabel ('Coefficients ', fontsize =20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_2_'></a>[Tuning the model through CV](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again to decide on a $\\lambda$ value we perform hyperparameter tuning through cross-validation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassoCV = LassoCV(alphas=lambdas, \n",
    "                       cv=5)\n",
    "lassoCV.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `LassoCV` could alternatively take the number of lambdas instead of a list of values:\n",
    "\n",
    "````{python}\n",
    "    lassoCV = LassoCV(n_alphas=100, \n",
    "                       cv=5)\n",
    "\n",
    "````\n",
    "This is not an option in `RidgeCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we run cross-validation, we can retrieve the best `lambda`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassoCV.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassoCV.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see variable selection feature in LASSO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(lassoCV.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LassoCV` also stores the MSE paths for the cross-validated resamplings for each value of lambda (again this is not the case in `RidgeCV`). Let us create a dataframe with all the five values of the MSE across lambda iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lassoCV.mse_path_,\n",
    "             index = ['log(lambda)='+str(round(np.log10(num),3)) for num in lambdas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the mean for each the 5 folds across all lambda values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lassoCV.mse_path_).mean(1)\n",
    "# mean(1) indicates to average values across columns. \n",
    "# Change to 0 and see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And get index in the array for the minimum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_min_mse = np.argmin(pd.DataFrame(lassoCV.mse_path_).mean(1))\n",
    "lasso_min_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which allow us to retrieve the best lambda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassoCV.alphas_[lasso_min_mse]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it corresponds to the optimal value returned by `LassoCV.alpha`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_3_'></a>[Test error estimation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can get the MSE and $R^2$ for the training and the test set. First, the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassoCV.score(X_train,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 can also be obtained as\n",
    "r2_score(y_train,lassoCV.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_train,lassoCV.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassoCV.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test,lassoCV.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test,lassoCV.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We accumulate this values to our lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.append('LASSO')\n",
    "r2_train.append(lassoCV.score(X_train,y_train))\n",
    "r2_test.append(lassoCV.score(X_test,y_test))\n",
    "mse_train.append(mean_squared_error(y_train,lassoCV.predict(X_train)))\n",
    "mse_test.append(mean_squared_error(y_test,lassoCV.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us produce a final table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'R2_train':r2_train,\n",
    "                        'R2_test':r2_test,\n",
    "                        'mse_train':mse_train,\n",
    "                        'mse_test':mse_test},\n",
    "                        index=model)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Interpret the above results in terms of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[YOUR TURN](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the regression using ElasticNetCV. In this example, we will cross-validate both the regularization penalty ($\\lambda$) for three different values of the mix of Lasso-Ridge regularization.\n",
    "\n",
    "1. Create three different Elastic Net instances with three values for (\\alpha) (between 0 and 1).\n",
    "2. Pass the set of ($\\lambda$) values already created to these instances.\n",
    "3. Complete the table with the results from these three models.\n",
    "4. Reach a conclusion on the best predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[YOUR TURN 2](#toc0_)\n",
    "\n",
    "This is a more complex (and optional) task.\n",
    "\n",
    "**Goal**: demonstrate the bias of LASSO regression in a high-dimensional linear regression. To do so implement the following steps:\n",
    "\n",
    "**1. Bootstrapping the training sample:**\n",
    "*   Perform the following steps a large number of times (e.g., B = 1000):\n",
    "    +  Resample: Draw a random sample with replacement from the original dataset (bootstrapping).\n",
    "    +  Fit LASSO: \n",
    "        *   Fit the LASSO regression model to the bootstrapped sample. \n",
    "        *   Determine the optimal tuning parameter (lambda) using cross-validation within the bootstrapped sample.\n",
    "    +  Obtain Coefficients: Extract the estimated coefficients (β_hat_b) from the fitted LASSO model.\n",
    "\n",
    "**2. Analyze the results:**\n",
    "*   Empirical distribution:\n",
    "    *   For each feature (j), you now have B estimates of its coefficient (β_hat_b,j). \n",
    "    *   Calculate the mean and standard deviation of these B estimates.\n",
    "*   Compare to True Coefficients:\n",
    "    *   Bias: Compare the mean of the bootstrapped coefficient estimates (mean(β_hat_b,j)) to the true coefficient (β_true,j). \n",
    "        *   If LASSO is biased, you'll observe systematic differences between the mean estimates and the true values.\n",
    "    *   Variance: Examine the standard deviation of the bootstrapped coefficient estimates. This gives you an idea of the variability of the LASSO estimates.\n",
    "\n",
    "**3. Visualization**:\n",
    "*   Plot the results:\n",
    "    *   Create scatter plots of the true coefficients (β_true) versus the mean of the bootstrapped coefficients (mean(β_hat_b)) for each feature. \n",
    "    *   Ideally, you'd see a strong linear relationship with a slope of 1 if LASSO were unbiased. Deviations from this line indicate bias.\n",
    "\n",
    "**Key Considerations:**\n",
    "\n",
    "*   **Sparsity:** The degree of sparsity in the true model will significantly impact LASSO's performance and bias (it can reduce bias for strong predictors and incerase bias for weak predictors).\n",
    "*   **Signal-to-noise Ratio:** Higher noise levels can increase the bias of LASSO.\n",
    "*   **Choice of tuning parameter:** The selection of the tuning parameter (lambda) in LASSO is crucial. Cross-validation within each bootstrap sample is recommended.\n",
    "*   **Number of bootstrap replicates (B):** A larger number of bootstrap replicates will provide more stable estimates of bias and variability.\n",
    "\n",
    "\n",
    "**Note:** While this approach demonstrates the potential for bias, it's important to remember that LASSO's primary advantage lies in its ability to perform variable selection and improve prediction accuracy in high-dimensional settings, even if it introduces some bias in the coefficient estimates.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigDataEcon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
