{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric and non-parametric methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main takeaways:\n",
    "\n",
    "1. Parametric regression works with smaller datasets, whereas nonparametric regression need larger datasets to capture patterns\n",
    "2. In both cases there is some point at which increasing the level of complexity in the model leads to overfitting\n",
    "3. Other things equal, nonparametric methods allow for more complexity before overfitting the larger the dataset  \n",
    "4. The flexibility of a learner (its ability to capture different functional forms) is related to its functional class $f$: more flexible models are more complex models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading libraries and generating a synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-3, 3, 50).reshape(-1,1)  # Feature is reshaped to create a 2D array with one column!\n",
    "y = np.sin(X[:,0]) + np.random.normal(0, 0.8, X.shape[0])  # True function + noise. Note that X is a 2D array so we must pick one columns \n",
    "# y = np.sin(X).ravel() + np.random.normal(0, 0.8, X.shape[0])  # Alternatively one can flatten the 2D array with ravel  \n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parametric regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- PARAMETRIC ESTIMATION ----\n",
    "plt.figure(figsize=(12, 5))\n",
    "degrees = [1, 3,15]  # Polynomial degrees\n",
    "\n",
    "for i, d in enumerate(degrees, 1):\n",
    "    poly = PolynomialFeatures(degree=d)\n",
    "    X_poly = poly.fit_transform(X_train)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y_train)\n",
    "    \n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    y_pred = model.predict(X_test_poly)\n",
    "    \n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.scatter(X_train, y_train, color='gray', label='Train Data')\n",
    "    plt.scatter(X_test, y_test, color='red', label='Test Data')\n",
    "    plt.plot(np.sort(X_test.ravel()), y_pred[np.argsort(X_test.ravel())], label=f'Degree {d}', lw=2)\n",
    "    plt.plot(X, np.sin(X), label='True Function', linestyle='dashed', color='black')\n",
    "    plt.title(f'Polynomial Regression (Degree {d})')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Non-parametric: regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- NONPARAMETRIC: Regression Tree ----\n",
    "plt.figure(figsize=(12, 5))\n",
    "max_depths = [1, 3, 5]\n",
    "\n",
    "for i, depth in enumerate(max_depths, 1):\n",
    "    tree = DecisionTreeRegressor(max_depth=depth)\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = tree.predict(X_test)  # Predict over full X for visualization\n",
    "    \n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.scatter(X_train, y_train, color='gray', label='Train Data')\n",
    "    plt.scatter(X_test, y_test, color='red', label='Test Data')\n",
    "    plt.plot(X, np.sin(X), label='True Function', linestyle='dashed', color='black')\n",
    "    plt.plot(np.sort(X_test.ravel()), y_pred[np.argsort(X_test.ravel())], label=f'Regression Tree (depth={depth})', lw=2)\n",
    "    plt.title(f'Regression Tree with depth={depth}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sample size in non-parametric regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bigger dataset\n",
    "# # Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-3, 3, 500).reshape(-1,1)  # Feature\n",
    "y = np.sin(X).ravel() + np.random.normal(0, .8, X.shape[0])  # True function + noise\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for i, depth in enumerate(max_depths, 1):\n",
    "    tree = DecisionTreeRegressor(max_depth=depth)\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = tree.predict(X_test)  # Predict over full X for visualization\n",
    "    \n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.scatter(X_train, y_train, color='gray', label='Train Data',alpha=0.2)\n",
    "    plt.scatter(X_test, y_test, color='red', label='Test Data',alpha=0.2)\n",
    "    plt.plot(X, np.sin(X), label='True Function', linestyle='dashed', color='black')\n",
    "    plt.plot(np.sort(X_test.ravel()), y_pred[np.argsort(X_test.ravel())], label=f'Regression Tree (depth={depth})', lw=2)\n",
    "    plt.title(f'Regression Tree with depth={depth}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even bigger dataset (n=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bigger dataset\n",
    "# # Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-3, 3, 50000).reshape(-1,1)  # Feature\n",
    "y = np.sin(X).ravel() + np.random.normal(0, .8, X.shape[0])  # True function + noise\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for i, depth in enumerate(max_depths, 1):\n",
    "    tree = DecisionTreeRegressor(max_depth=depth)\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = tree.predict(X_test)  # Predict over full X for visualization\n",
    "    \n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.scatter(X_train, y_train, color='gray', label='Train Data',alpha=0.05)\n",
    "    plt.scatter(X_test, y_test, color='red', label='Test Data',alpha=0.05)\n",
    "    plt.plot(X, np.sin(X), label='True Function', linestyle='dashed', color='black')\n",
    "    plt.plot(np.sort(X_test.ravel()), y_pred[np.argsort(X_test.ravel())], label=f'Regression Tree (depth={depth})', lw=2)\n",
    "    plt.title(f'Regression Tree with depth={depth}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigDataEcon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
